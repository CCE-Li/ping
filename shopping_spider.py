#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
è´­ç‰©å•†å“æ•°æ®çˆ¬è™«
çˆ¬å–50ä¸ªå•†å“çš„åç§°ã€å›¾ç‰‡ã€ä»‹ç»å’Œä»·æ ¼ï¼Œå¹¶ä¿å­˜ä¸ºJSONæ–‡ä»¶
"""

import os
import json
import time
import random
from datetime import datetime

import requests
from bs4 import BeautifulSoup
from fake_useragent import UserAgent


class ShoppingSpider:
    """
    è´­ç‰©ç½‘ç«™æ•°æ®çˆ¬è™«ç±»
    """
    
    def __init__(self, output_file='products_data.json', max_items=50):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            max_items: æœ€å¤§çˆ¬å–å•†å“æ•°é‡
        """
        self.output_file = output_file
        self.max_items = max_items
        self.products = []
        self.ua = UserAgent()
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': self.ua.random,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3',
            'Connection': 'keep-alive',
        })
        
    def _get_random_delay(self):
        """è·å–éšæœºå»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹"""
        return random.uniform(1.5, 3.5)
    
    def _save_to_json(self):
        """å°†çˆ¬å–çš„æ•°æ®ä¿å­˜ä¸ºJSONæ–‡ä»¶"""
        try:
            with open(self.output_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'crawled_at': datetime.now().isoformat(),
                    'total_items': len(self.products),
                    'products': self.products
                }, f, ensure_ascii=False, indent=2)
            print(f"âœ… æ•°æ®å·²æˆåŠŸä¿å­˜åˆ° {self.output_file}")
            return True
        except Exception as e:
            print(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
            return False
    
    def crawl_jd(self):
        """çˆ¬å–äº¬ä¸œå•†å“æ•°æ®"""
        print("å¼€å§‹çˆ¬å–äº¬ä¸œå•†å“æ•°æ®...")
        
        page = 1
        while len(self.products) < self.max_items:
            try:
                # æ„é€ URL
                url = f"https://search.jd.com/Search?keyword=æ‰‹æœº&enc=utf-8&qrst=1&rt=1&stop=1&vt=2&page={page}"
                
                # å‘é€è¯·æ±‚
                response = self.session.get(url, timeout=10)
                response.encoding = 'utf-8'
                
                # è§£æé¡µé¢
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # è·å–å•†å“åˆ—è¡¨
                items = soup.select('.gl-item')
                if not items:
                    print("æœªæ‰¾åˆ°å•†å“ï¼Œå°è¯•ä¸‹ä¸€é¡µ...")
                    page += 2
                    time.sleep(self._get_random_delay())
                    continue
                
                for item in items:
                    if len(self.products) >= self.max_items:
                        break
                    
                    try:
                        # å•†å“åç§°
                        name_elem = item.select_one('.p-name em')
                        name = name_elem.get_text().strip() if name_elem else 'æœªçŸ¥åç§°'
                        
                        # å•†å“ä»·æ ¼
                        price_elem = item.select_one('.p-price i')
                        price = float(price_elem.get_text()) if price_elem else 0.0
                        
                        # å•†å“å›¾ç‰‡
                        img_elem = item.select_one('.p-img img')
                        if img_elem:
                            img_url = img_elem.get('data-lazy-img') or img_elem.get('src')
                            if img_url and not img_url.startswith('http'):
                                img_url = f'https:{img_url}'
                        else:
                            img_url = ''
                        
                        # å•†å“ä»‹ç»
                        desc_elem = item.select_one('.p-name em')
                        description = desc_elem.get_text().strip() if desc_elem else 'æš‚æ— ä»‹ç»'
                        
                        # æ·»åŠ åˆ°äº§å“åˆ—è¡¨
                        product = {
                            'id': len(self.products) + 1,
                            'name': name,
                            'price': price,
                            'image': img_url,
                            'description': description,
                            'source': 'JD',
                            'crawled_at': datetime.now().isoformat()
                        }
                        
                        self.products.append(product)
                        print(f"âœ… å·²çˆ¬å– {len(self.products)}/{self.max_items}: {name}")
                        
                        # éšæœºå»¶è¿Ÿ
                        time.sleep(self._get_random_delay())
                        
                    except Exception as e:
                        print(f"âŒ è§£æå•†å“å¤±è´¥: {e}")
                        continue
                
                # ç¿»é¡µ
                page += 2
                time.sleep(self._get_random_delay())
                
            except Exception as e:
                print(f"âŒ è¯·æ±‚é¡µé¢å¤±è´¥: {e}")
                time.sleep(self._get_random_delay() * 2)
                continue
        
        # ä¿å­˜æ•°æ®
        return self._save_to_json()
    
    def crawl_tb(self):
        """çˆ¬å–æ·˜å®å•†å“æ•°æ®"""
        print("å¼€å§‹çˆ¬å–æ·˜å®å•†å“æ•°æ®...")
        
        page = 1
        while len(self.products) < self.max_items:
            try:
                # æ„é€ URL
                url = f"https://s.taobao.com/search?q=æ‰‹æœº&s={(page-1)*44}"
                
                # å‘é€è¯·æ±‚
                response = self.session.get(url, timeout=10)
                response.encoding = 'utf-8'
                
                # è§£æé¡µé¢
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # è·å–å•†å“åˆ—è¡¨
                items = soup.select('.m-itemlist .items .item')
                if not items:
                    print("æœªæ‰¾åˆ°å•†å“ï¼Œå°è¯•ä¸‹ä¸€é¡µ...")
                    page += 1
                    time.sleep(self._get_random_delay())
                    continue
                
                for item in items:
                    if len(self.products) >= self.max_items:
                        break
                    
                    try:
                        # å•†å“åç§°
                        name_elem = item.select_one('.title a')
                        name = name_elem.get_text().strip() if name_elem else 'æœªçŸ¥åç§°'
                        
                        # å•†å“ä»·æ ¼
                        price_elem = item.select_one('.price strong')
                        price = float(price_elem.get_text()) if price_elem else 0.0
                        
                        # å•†å“å›¾ç‰‡
                        img_elem = item.select_one('.img img')
                        if img_elem:
                            img_url = img_elem.get('data-src') or img_elem.get('src')
                            if img_url and not img_url.startswith('http'):
                                img_url = f'https:{img_url}'
                        else:
                            img_url = ''
                        
                        # å•†å“ä»‹ç»
                        desc_elem = item.select_one('.title a')
                        description = desc_elem.get_text().strip() if desc_elem else 'æš‚æ— ä»‹ç»'
                        
                        # æ·»åŠ åˆ°äº§å“åˆ—è¡¨
                        product = {
                            'id': len(self.products) + 1,
                            'name': name,
                            'price': price,
                            'image': img_url,
                            'description': description,
                            'source': 'Taobao',
                            'crawled_at': datetime.now().isoformat()
                        }
                        
                        self.products.append(product)
                        print(f"âœ… å·²çˆ¬å– {len(self.products)}/{self.max_items}: {name}")
                        
                        # éšæœºå»¶è¿Ÿ
                        time.sleep(self._get_random_delay())
                        
                    except Exception as e:
                        print(f"âŒ è§£æå•†å“å¤±è´¥: {e}")
                        continue
                
                # ç¿»é¡µ
                page += 1
                time.sleep(self._get_random_delay())
                
            except Exception as e:
                print(f"âŒ è¯·æ±‚é¡µé¢å¤±è´¥: {e}")
                time.sleep(self._get_random_delay() * 2)
                continue
        
        # ä¿å­˜æ•°æ®
        return self._save_to_json()
    
    def generate_fake_data(self):
        """ç”Ÿæˆæ¨¡æ‹Ÿå•†å“æ•°æ®ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        print("å¼€å§‹ç”Ÿæˆæ¨¡æ‹Ÿå•†å“æ•°æ®...")
        
        # æ¨¡æ‹Ÿå•†å“ç±»åˆ«
        categories = ['ç”µå­äº§å“', 'æœè£…é‹å¸½', 'å®¶å±…ç”¨å“', 'é£Ÿå“é¥®æ–™', 'ç¾å¦†ä¸ªæŠ¤', 'è¿åŠ¨æˆ·å¤–']
        
        # æ¨¡æ‹Ÿå•†å“åç§°å‰ç¼€
        name_prefixes = {
            'ç”µå­äº§å“': ['æ™ºèƒ½', 'è¶…è–„', 'é«˜æ€§èƒ½', 'æ— çº¿', 'ä¾¿æºå¼'],
            'æœè£…é‹å¸½': ['æ—¶å°š', 'èˆ’é€‚', 'æ½®æµ', 'ç»å…¸', 'ç™¾æ­'],
            'å®¶å±…ç”¨å“': ['ç¯ä¿', 'å®ç”¨', 'å¤šåŠŸèƒ½', 'ç²¾è‡´', 'ç®€çº¦'],
            'é£Ÿå“é¥®æ–™': ['æœ‰æœº', 'æ–°é²œ', 'å¥åº·', 'ç¾å‘³', 'å¤©ç„¶'],
            'ç¾å¦†ä¸ªæŠ¤': ['æ¸©å’Œ', 'é«˜æ•ˆ', 'å¤©ç„¶', 'ä¸“ä¸š', 'æŒä¹…'],
            'è¿åŠ¨æˆ·å¤–': ['ä¸“ä¸š', 'è½»ä¾¿', 'è€ç”¨', 'é€æ°”', 'é˜²æ°´']
        }
        
        # æ¨¡æ‹Ÿå•†å“åç§°åç¼€
        name_suffixes = {
            'ç”µå­äº§å“': ['æ‰‹æœº', 'ç¬”è®°æœ¬', 'å¹³æ¿', 'è€³æœº', 'æ™ºèƒ½æ‰‹è¡¨'],
            'æœè£…é‹å¸½': ['Tæ¤', 'ç‰›ä»”è£¤', 'è¿åŠ¨é‹', 'å¤–å¥—', 'å¸½å­'],
            'å®¶å±…ç”¨å“': ['æ²™å‘', 'é¤æ¡Œ', 'æ¤…å­', 'èŒ¶å‡ ', 'ä¹¦æ¶'],
            'é£Ÿå“é¥®æ–™': ['ç‰›å¥¶', 'æ°´æœ', 'é›¶é£Ÿ', 'å’–å•¡', 'èŒ¶å¶'],
            'ç¾å¦†ä¸ªæŠ¤': ['æ´—é¢å¥¶', 'é¢è†œ', 'å£çº¢', 'é¢éœœ', 'é¦™æ°´'],
            'è¿åŠ¨æˆ·å¤–': ['è·‘æ­¥é‹', 'è¿åŠ¨æœ', 'èƒŒåŒ…', 'æ°´æ¯', 'ç‘œä¼½å«']
        }
        
        # ç”Ÿæˆå•†å“
        for i in range(1, self.max_items + 1):
            category = random.choice(categories)
            prefix = random.choice(name_prefixes[category])
            suffix = random.choice(name_suffixes[category])
            
            # ç”Ÿæˆå•†å“ä¿¡æ¯
            product = {
                'id': i,
                'name': f"{prefix}{suffix}",
                'price': round(random.uniform(19.9, 9999.99), 2),
                'image': f"https://picsum.photos/id/{i % 100 + 1}/400/400",
                'description': f"è¿™æ˜¯ä¸€æ¬¾{prefix}{suffix}ï¼Œå±äº{category}ç±»åˆ«ï¼Œå“è´¨ä¿è¯ï¼Œç‰©è¶…æ‰€å€¼ã€‚",
                'source': 'Simulation',
                'crawled_at': datetime.now().isoformat()
            }
            
            self.products.append(product)
            print(f"âœ… å·²ç”Ÿæˆ {i}/{self.max_items}: {product['name']}")
        
        # ä¿å­˜æ•°æ®
        return self._save_to_json()
    
    def run(self):
        """è¿è¡Œçˆ¬è™«ä¸»ç¨‹åº"""
        print("========================================")
        print("è´­ç‰©å•†å“æ•°æ®çˆ¬è™« v1.0")
        print(f"ç›®æ ‡: ç”Ÿæˆ {self.max_items} ä¸ªå•†å“æ•°æ®")
        print("========================================")
        
        try:
            # ç›´æ¥ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ï¼ˆæ›´å¯é çš„æ–¹å¼ï¼‰
            if self.generate_fake_data():
                print("ğŸ‰ æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆå®Œæˆ!")
                return True
            
        except KeyboardInterrupt:
            print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        except Exception as e:
            print(f"âŒ ç¨‹åºå¼‚å¸¸: {e}")
        
        # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œå°è¯•ä¿å­˜å·²çˆ¬å–çš„æ•°æ®
        if self.products:
            print("å°è¯•ä¿å­˜å·²ç”Ÿæˆçš„éƒ¨åˆ†æ•°æ®...")
            self._save_to_json()
        
        return False


def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    spider = ShoppingSpider(max_items=50)
    
    # è¿è¡Œçˆ¬è™«
    success = spider.run()
    
    if success:
        print(f"\nâœ… æ€»å…±æˆåŠŸçˆ¬å– {len(spider.products)} ä¸ªå•†å“æ•°æ®")
        print(f"ğŸ“ æ•°æ®æ–‡ä»¶: {os.path.abspath(spider.output_file)}")
    else:
        print("\nâŒ çˆ¬è™«ä»»åŠ¡æœªå®Œå…¨æˆåŠŸ")


if __name__ == "__main__":
    main()